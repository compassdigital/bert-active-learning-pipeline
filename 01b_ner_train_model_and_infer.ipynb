{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38dc48ab",
   "metadata": {},
   "source": [
    "This notebook takes the labelled entities provided from the user, trains a model and applies it to the full dataset. At the end of this notebook, you will have every item's itemname_col decomposed into its entities.\n",
    "\n",
    "Expected input:\n",
    "- NER for tagging\n",
    "- Preprocessed files\n",
    "\n",
    "Expected output:\n",
    "- Saved NER model\n",
    "- Inference output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7439621c-b792-4632-8b4d-848ef3dabf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/shared_sandbox/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (11.0.0), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "2023-02-22 14:49:18.334799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 14:49:18.969467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 14:49:18.969529: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 14:49:18.969535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/shared/code/08_protein_attribution/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import pickle\n",
    "import hjson as json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eba7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our params file\n",
    "f = open('input_params.hjson')\n",
    "params = json.load(f)\n",
    "f.close()\n",
    "\n",
    "# Modelling params\n",
    "itemname_col = params['core']['itemname_col']\n",
    "# training_sets = params['core']['training_sets']\n",
    "tag_lookup = params['core']['tag_lookup']\n",
    "model_name = params['core']['ner_model_name']\n",
    "\n",
    "# Algorithm specific params\n",
    "use_pretrained_model = params['core']['use_pretrained_model']\n",
    "model_architecture = params['core']['model_architecture']\n",
    "model_path = params['core']['model_path']\n",
    "batch_size = params['nb_two']['batch_size']\n",
    "learning_rate = params['nb_two']['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2d6503-6971-49b8-a684-6543949f32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:00, 4065.82it/s]\n",
      "2023-02-22 14:49:21.314902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.315258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.343684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.344023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.344989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.345307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.346310: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 14:49:21.712701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.713018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.713885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.714174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.715024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:21.715312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.048185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.048520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.049475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.049783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.050376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44153 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "2023-02-22 14:49:23.051065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 44153 MB memory:  -> device: 1, name: Quadro RTX 8000, pci bus id: 0000:0a:00.0, compute capability: 7.5\n",
      "2023-02-22 14:49:23.051319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.051593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 44153 MB memory:  -> device: 2, name: Quadro RTX 8000, pci bus id: 0000:42:00.0, compute capability: 7.5\n",
      "2023-02-22 14:49:23.051837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-22 14:49:23.052144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 44081 MB memory:  -> device: 3, name: Quadro RTX 8000, pci bus id: 0000:43:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some layers from the model checkpoint at epoch_95 were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at epoch_95 and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 14:49:41.240375: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x595cce60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-22 14:49:41.240409: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro RTX 8000, Compute Capability 7.5\n",
      "2023-02-22 14:49:41.240414: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Quadro RTX 8000, Compute Capability 7.5\n",
      "2023-02-22 14:49:41.240417: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): Quadro RTX 8000, Compute Capability 7.5\n",
      "2023-02-22 14:49:41.240421: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): Quadro RTX 8000, Compute Capability 7.5\n",
      "2023-02-22 14:49:41.244842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-22 14:49:41.310448: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-22 14:49:41.349004: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 31s 173ms/step - loss: 0.5210 - accuracy: 0.8068 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.2023 - accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.1074 - accuracy: 0.9606 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.0770 - accuracy: 0.9712 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.0473 - accuracy: 0.9841 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0189 - accuracy: 0.9947 - lr: 5.0000e-05\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0224 - accuracy: 0.9955 - lr: 5.0000e-05\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0116 - accuracy: 0.9977 - lr: 5.0000e-05\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0124 - accuracy: 0.9977 - lr: 5.0000e-05\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0117 - accuracy: 0.9970 - lr: 5.0000e-05\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0029 - accuracy: 1.0000 - lr: 2.5000e-05\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 2.5000e-05\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.0017 - accuracy: 1.0000 - lr: 2.5000e-05\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0021 - accuracy: 1.0000 - lr: 2.5000e-05\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 2.5000e-05\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 1.2500e-05\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 1.2500e-05\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 1.2500e-05\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 1.2500e-05\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 1.2500e-05\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 6.2500e-06\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 6.2500e-06\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 6.2500e-06\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 6.2500e-06\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 6.2500e-06\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 3.1250e-06\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 3.1250e-06\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 3.1250e-06\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 3.1250e-06\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 9.8725e-04 - accuracy: 1.0000 - lr: 3.1250e-06\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 1.5625e-06\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 1.5625e-06\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 9.1739e-04 - accuracy: 1.0000 - lr: 1.5625e-06\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 9.6239e-04 - accuracy: 1.0000 - lr: 1.5625e-06\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 9.3649e-04 - accuracy: 1.0000 - lr: 1.5625e-06\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 9.6301e-04 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 9.6776e-04 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 9.9295e-04 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 9.6650e-04 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 9.3770e-04 - accuracy: 1.0000 - lr: 7.8125e-07\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 9.7247e-04 - accuracy: 1.0000 - lr: 3.9062e-07\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 9.8942e-04 - accuracy: 1.0000 - lr: 3.9062e-07\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 9.5275e-04 - accuracy: 1.0000 - lr: 3.9062e-07\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0066 - accuracy: 0.9992 - lr: 3.9062e-07\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 3.9062e-07\n"
     ]
    }
   ],
   "source": [
    "# Train the NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_architecture)\n",
    "bertPreproc = ut.BERTPreprocess(tokenizer)\n",
    "X_train, Y_train, schema = ut.loadPreprocess(model_name, 'item_for_selection', \"_\", bertPreproc)\n",
    "model = ut.loadTrainModel(schema, X_train, Y_train, use_pretrained_model=use_pretrained_model, model_path=model_path, model_architecture = model_architecture, batch_size = batch_size, learning_rate = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1ef523-e9c1-4722-8e54-c1435cd84720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineitem_name</th>\n",
       "      <th>tier_1</th>\n",
       "      <th>tier_2</th>\n",
       "      <th>tier_3</th>\n",
       "      <th>tier_4</th>\n",
       "      <th>sales_amt_gross</th>\n",
       "      <th>item_for_selection</th>\n",
       "      <th>clust_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grits Large</td>\n",
       "      <td>Food</td>\n",
       "      <td>Breakfast</td>\n",
       "      <td>Breakfast Side</td>\n",
       "      <td>Grits</td>\n",
       "      <td>78552.240000</td>\n",
       "      <td>grits large</td>\n",
       "      <td>grits large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Boat</td>\n",
       "      <td>Food</td>\n",
       "      <td>Entree</td>\n",
       "      <td>Weighed/Build Your Own</td>\n",
       "      <td>Weighed/Build Your Own</td>\n",
       "      <td>190793.963333</td>\n",
       "      <td>large boat</td>\n",
       "      <td>large boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kitchen Fresh 1137 Italian Focaccia (7.7oz)</td>\n",
       "      <td>Food</td>\n",
       "      <td>Entree</td>\n",
       "      <td>Sandwich/Wrap</td>\n",
       "      <td>Sandwich/Wrap</td>\n",
       "      <td>84461.820000</td>\n",
       "      <td>kitchen fresh 1137 italian focaccia  7 7oz</td>\n",
       "      <td>kitchen fresh 1137 italian focaccia  7 7oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adobo Chicken Bowl</td>\n",
       "      <td>Food</td>\n",
       "      <td>Entree</td>\n",
       "      <td>Noodle/Grain Bowl</td>\n",
       "      <td>Noodle/Grain Bowl</td>\n",
       "      <td>236852.690000</td>\n",
       "      <td>adobo chicken bowl</td>\n",
       "      <td>adobo chicken bowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8oz Steel Cut Oatmeal (1</td>\n",
       "      <td>Food</td>\n",
       "      <td>Breakfast</td>\n",
       "      <td>Oatmeal/Cereal</td>\n",
       "      <td>Oatmeal</td>\n",
       "      <td>125298.810000</td>\n",
       "      <td>8oz steel cut oatmeal  1</td>\n",
       "      <td>8oz steel cut oatmeal  1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460910</th>\n",
       "      <td>Spicy Pepperjack Burger</td>\n",
       "      <td>Food</td>\n",
       "      <td>Entree</td>\n",
       "      <td>Burger</td>\n",
       "      <td>Burger</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>spicy pepperjack burger</td>\n",
       "      <td>spicy pepperjack burger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460911</th>\n",
       "      <td>Charlotte SP White Egg Salad (6oz)</td>\n",
       "      <td>Food</td>\n",
       "      <td>Entree</td>\n",
       "      <td>Other Entree</td>\n",
       "      <td>Other Entree</td>\n",
       "      <td>2.990000</td>\n",
       "      <td>charlotte sp white egg salad  6oz</td>\n",
       "      <td>charlotte sp white egg salad  6oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460912</th>\n",
       "      <td>Crisper and Waffle Combo</td>\n",
       "      <td>Food</td>\n",
       "      <td>Breakfast</td>\n",
       "      <td>Griddle</td>\n",
       "      <td>Waffles</td>\n",
       "      <td>11.890000</td>\n",
       "      <td>crisper and waffle combo</td>\n",
       "      <td>crisper and waffle combo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460913</th>\n",
       "      <td>BFK - FIT Applewood Bacon Egg &amp; Cheddar Flatbread</td>\n",
       "      <td>Food</td>\n",
       "      <td>Breakfast</td>\n",
       "      <td>Breakfast Sandwiches</td>\n",
       "      <td>Breakfast Sandwich/Wrap</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>bfk   fit applewood bacon egg   cheddar flatbread</td>\n",
       "      <td>bfk   fit applewood bacon egg   cheddar flatbread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460914</th>\n",
       "      <td>Freekah&amp; Delicata Squash&amp; Pomegranate &amp; Pumpki...</td>\n",
       "      <td>Food</td>\n",
       "      <td>Entree</td>\n",
       "      <td>Salad</td>\n",
       "      <td>Salad</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>freekah  delicata squash  pomegranate   pumpki...</td>\n",
       "      <td>freekah  delicata squash  pomegranate   pumpki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460915 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            lineitem_name tier_1     tier_2  \\\n",
       "0                                             Grits Large   Food  Breakfast   \n",
       "1                                              Large Boat   Food     Entree   \n",
       "2             Kitchen Fresh 1137 Italian Focaccia (7.7oz)   Food     Entree   \n",
       "3                                      Adobo Chicken Bowl   Food     Entree   \n",
       "4                                8oz Steel Cut Oatmeal (1   Food  Breakfast   \n",
       "...                                                   ...    ...        ...   \n",
       "460910                            Spicy Pepperjack Burger   Food     Entree   \n",
       "460911                 Charlotte SP White Egg Salad (6oz)   Food     Entree   \n",
       "460912                           Crisper and Waffle Combo   Food  Breakfast   \n",
       "460913  BFK - FIT Applewood Bacon Egg & Cheddar Flatbread   Food  Breakfast   \n",
       "460914  Freekah& Delicata Squash& Pomegranate & Pumpki...   Food     Entree   \n",
       "\n",
       "                        tier_3                   tier_4  sales_amt_gross  \\\n",
       "0               Breakfast Side                    Grits     78552.240000   \n",
       "1       Weighed/Build Your Own   Weighed/Build Your Own    190793.963333   \n",
       "2                Sandwich/Wrap            Sandwich/Wrap     84461.820000   \n",
       "3            Noodle/Grain Bowl        Noodle/Grain Bowl    236852.690000   \n",
       "4               Oatmeal/Cereal                  Oatmeal    125298.810000   \n",
       "...                        ...                      ...              ...   \n",
       "460910                  Burger                   Burger         5.490000   \n",
       "460911            Other Entree             Other Entree         2.990000   \n",
       "460912                 Griddle                  Waffles        11.890000   \n",
       "460913    Breakfast Sandwiches  Breakfast Sandwich/Wrap         0.000000   \n",
       "460914                   Salad                    Salad        10.250000   \n",
       "\n",
       "                                       item_for_selection  \\\n",
       "0                                             grits large   \n",
       "1                                              large boat   \n",
       "2              kitchen fresh 1137 italian focaccia  7 7oz   \n",
       "3                                      adobo chicken bowl   \n",
       "4                                8oz steel cut oatmeal  1   \n",
       "...                                                   ...   \n",
       "460910                            spicy pepperjack burger   \n",
       "460911                  charlotte sp white egg salad  6oz   \n",
       "460912                           crisper and waffle combo   \n",
       "460913  bfk   fit applewood bacon egg   cheddar flatbread   \n",
       "460914  freekah  delicata squash  pomegranate   pumpki...   \n",
       "\n",
       "                                              clust_input  \n",
       "0                                             grits large  \n",
       "1                                              large boat  \n",
       "2              kitchen fresh 1137 italian focaccia  7 7oz  \n",
       "3                                      adobo chicken bowl  \n",
       "4                                8oz steel cut oatmeal  1  \n",
       "...                                                   ...  \n",
       "460910                            spicy pepperjack burger  \n",
       "460911                  charlotte sp white egg salad  6oz  \n",
       "460912                           crisper and waffle combo  \n",
       "460913  bfk   fit applewood bacon egg   cheddar flatbread  \n",
       "460914  freekah  delicata squash  pomegranate   pumpki...  \n",
       "\n",
       "[460915 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use our trained model to infer on the full catalog\n",
    "item_df = pd.read_csv(f'named_entity_recognition/{model_name}/data/{model_name}_preprocessed.csv')\n",
    "cols = list(item_df)\n",
    "results = ut.modelInferAndFormat(item_df, itemname_col, bertPreproc, schema, model, tag_lookup)\n",
    "\n",
    "# Replace the clust_input with the NER results\n",
    "results['clust_input'] = results['protein']\n",
    "item_df.drop(columns=['clust_input'], inplace=True)\n",
    "item_df = item_df.merge(results[['clust_input', itemname_col]], on=itemname_col, how='left').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f73cce99-cf8b-4698-8ea4-12102db10c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results \n",
    "item_df[cols].to_csv(f\"named_entity_recognition/{model_name}/data/{model_name}_round1results.csv\",index = False)\n",
    "model.save_pretrained(f'named_entity_recognition/{model_name}/models/{model_name}')\n",
    "\n",
    "out_dict = {\n",
    "    'vec_modelpath': model_path,\n",
    "    'schema': schema\n",
    "}\n",
    "pickle.dump(out_dict, open(f\"named_entity_recognition/{model_name}/models/{model_name}_model_params.p\",'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b66850be",
   "metadata": {},
   "source": [
    "#### Validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab638e8d-93f9-40dd-9e7b-e54bc5107756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9    458662\n",
       "Name: rounded_confidence, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This section is for exploring model results\n",
    "\n",
    "results['rounded_confidence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cdb58e0-ad58-4c73-9749-8411dc8a0e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineitem_name</th>\n",
       "      <th>protein</th>\n",
       "      <th>not_protein</th>\n",
       "      <th>rounded_confidence</th>\n",
       "      <th>clust_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grits Large</td>\n",
       "      <td></td>\n",
       "      <td>Grits Large</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290452</th>\n",
       "      <td>Classic Greek (Dairy)</td>\n",
       "      <td></td>\n",
       "      <td>Classic Greek (Dairy)</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290448</th>\n",
       "      <td>Soup - Tom Yum</td>\n",
       "      <td></td>\n",
       "      <td>Soup - Tom Yum</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290443</th>\n",
       "      <td>American Asian - Chicken Tenders</td>\n",
       "      <td></td>\n",
       "      <td>American Asian - Chicken Tenders</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290439</th>\n",
       "      <td>Vegan Homestyle Chikn Tenders</td>\n",
       "      <td></td>\n",
       "      <td>Vegan Homestyle Chikn Tenders</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620632</th>\n",
       "      <td>Shrimp Chorizo and Bacon Taco 2</td>\n",
       "      <td></td>\n",
       "      <td>Shrimp Chorizo and Bacon Taco 2</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620629</th>\n",
       "      <td>Fricasse de Pollo</td>\n",
       "      <td></td>\n",
       "      <td>Fricasse de Pollo</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620626</th>\n",
       "      <td>WELL-DELI-Small Soup A</td>\n",
       "      <td></td>\n",
       "      <td>WELL-DELI-Small Soup A</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620683</th>\n",
       "      <td>Half Entree (3.50)</td>\n",
       "      <td></td>\n",
       "      <td>Half Entree (3.50)</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987999</th>\n",
       "      <td>Freekah&amp; Delicata Squash&amp; Pomegranate &amp; Pumpki...</td>\n",
       "      <td></td>\n",
       "      <td>Freekah&amp; Delicata Squash&amp; Pomegranate &amp; Pumpki...</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458662 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lineitem_name protein  \\\n",
       "0                                              Grits Large           \n",
       "1290452                              Classic Greek (Dairy)           \n",
       "1290448                                     Soup - Tom Yum           \n",
       "1290443                   American Asian - Chicken Tenders           \n",
       "1290439                      Vegan Homestyle Chikn Tenders           \n",
       "...                                                    ...     ...   \n",
       "620632                     Shrimp Chorizo and Bacon Taco 2           \n",
       "620629                                   Fricasse de Pollo           \n",
       "620626                              WELL-DELI-Small Soup A           \n",
       "620683                                  Half Entree (3.50)           \n",
       "1987999  Freekah& Delicata Squash& Pomegranate & Pumpki...           \n",
       "\n",
       "                                               not_protein  \\\n",
       "0                                              Grits Large   \n",
       "1290452                              Classic Greek (Dairy)   \n",
       "1290448                                     Soup - Tom Yum   \n",
       "1290443                   American Asian - Chicken Tenders   \n",
       "1290439                      Vegan Homestyle Chikn Tenders   \n",
       "...                                                    ...   \n",
       "620632                     Shrimp Chorizo and Bacon Taco 2   \n",
       "620629                                   Fricasse de Pollo   \n",
       "620626                              WELL-DELI-Small Soup A   \n",
       "620683                                  Half Entree (3.50)   \n",
       "1987999  Freekah& Delicata Squash& Pomegranate & Pumpki...   \n",
       "\n",
       "         rounded_confidence clust_input  \n",
       "0                       0.9              \n",
       "1290452                 0.9              \n",
       "1290448                 0.9              \n",
       "1290443                 0.9              \n",
       "1290439                 0.9              \n",
       "...                     ...         ...  \n",
       "620632                  0.9              \n",
       "620629                  0.9              \n",
       "620626                  0.9              \n",
       "620683                  0.9              \n",
       "1987999                 0.9              \n",
       "\n",
       "[458662 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ad36273-92fa-40c7-9c91-8d9dbbc8ec79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    458662\n",
       "Name: protein, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.protein.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead09536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared-sandbox",
   "language": "python",
   "name": "shared-sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
