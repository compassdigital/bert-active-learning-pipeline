{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c4870cf",
   "metadata": {},
   "source": [
    "This notebook is for querying the dataset and preprocessing the results\n",
    "\n",
    "Expected input: None\n",
    "\n",
    "Expected output: Cleaned dataset ready for NER or classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c95a5c8-95e4-4124-9504-62c03ba8fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/shared_sandbox/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (11.0.0), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "2023-02-22 05:28:50.444829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 05:28:51.070556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 05:28:51.070623: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 05:28:51.070630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/shared/code/08_protein_attribution/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils as ut\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import os\n",
    "import hjson as json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67e4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our params file\n",
    "f = open('input_params.hjson')\n",
    "params = json.load(f)\n",
    "f.close()\n",
    "\n",
    "# SQL params\n",
    "# db_type = params['core']['db_type']\n",
    "# sql_code = params['core']['sql_code']\n",
    "\n",
    "# Modelling params\n",
    "itemname_col = params['core']['itemname_col']\n",
    "spend_col = params['core']['spend_col']\n",
    "cls_input_cols = params['core']['cls_input_cols']\n",
    "\n",
    "ner_model_name = params['core']['ner_model_name']\n",
    "cls_model_name = params['core']['cls_model_name']\n",
    "model_type = 'classifier'\n",
    "ner_model_type = 'named_entity_recognition'\n",
    "sample_n = params['nb_three']['sample_n']\n",
    "test_samples = params['nb_three']['test_samples']\n",
    "n_strata = params['nb_three']['n_strata']\n",
    "ner_clust_cols = params['core']['ner_clust_cols']\n",
    "\n",
    "# # Algorithm specific params\n",
    "use_pretrained_model = params['core']['use_pretrained_model']\n",
    "model_architecture = params['core']['model_architecture']\n",
    "model_path = params['core']['model_path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aaf60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type = 'redshift'\n",
    "\n",
    "# The query must have the following: (a) an item name (b) a spend column\n",
    "sql_code = '''\n",
    "            select od.lineitem_name lineitem_name,\n",
    "            tier.cleansed_tier_1 tier_1,\n",
    "            tier.cleansed_tier_2 tier_2,\n",
    "            tier.cleansed_tier_3 tier_3,\n",
    "            tier.cleansed_tier_4 tier_4,\n",
    "            sum(od.lineitem_price) as sales_amt_gross\n",
    "            from \"cdl_bi_edw\".\"dbt_dev\".\"orders\" o\n",
    "            left join \"cdl_bi_edw\".\"dbt_dev\".\"order_details\" od on o.key = od.order_fk\n",
    "            left join \"cdl_bi_edw\".\"datascience_item_dim\".\"consolidated_item_attributes_dbt\" tier on od.lineitem_name = tier.item_name\n",
    "            where \n",
    "                o.order_fiscal_date_fk between '2019-01-01' and '2022-12-31'\n",
    "                and o.order_origin in ('pos', 'kiosk', 'mobile', 'web')\n",
    "                and o.transaction_type in ('Sale', 'Unknown', 'Other')\n",
    "                and o.check_outlier = 'No'\n",
    "                and tier2 in ('Breakfast', 'Entree')\n",
    "            group by lineitem_name, tier_1, tier_2, tier_3, tier_4\n",
    "         '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff8a0e4-f419-49a0-82d1-0e31ae6bba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_item_name(df, itemname_col):\n",
    "    '''\n",
    "    Add in any preprocessing your dataset needs in this step\n",
    "    \n",
    "    Other notes:\n",
    "    item_input will be used for inference\n",
    "    item_for_selection will be used for train and test set selection\n",
    "\n",
    "    '''\n",
    "\n",
    "    # # item_input is used for inference, for now we'll feed the model the input exactly as it's given to us.\n",
    "    # # You can do some processing to item_input if you find that it breaks the model.\n",
    "    # df['item_input'] = df[itemname_col]\n",
    "\n",
    "    # item_for_selection is setting up a groupby later on which is looking to group variations of the same item.\n",
    "    # Remove any special characters present in your item name here.\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) # Replace punctuation with ' '\n",
    "    df['item_for_selection']= df[itemname_col].fillna('').str.translate(translator).str.lower().str.strip()\n",
    "\n",
    "    #### Add in any preprocessing required here #####\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_column_names(df):\n",
    "    # Clean up our column names to replace spaces with underscores and remove caps\n",
    "    cols = list(df)\n",
    "    cols_new = []\n",
    "\n",
    "    for c in cols:\n",
    "        c = c.lower().strip()\n",
    "        c = c.replace(\" \", \"_\")\n",
    "        cols_new.append(c)\n",
    "\n",
    "    df.columns = cols_new\n",
    "    return df\n",
    "\n",
    "def create_folders_for_project(model_type, cls_model_name, ner_model_name):\n",
    "    # Create folders for this project if they don't exist\n",
    "\n",
    "    if not os.path.exists(f'{model_type}/{cls_model_name}/data'):\n",
    "        os.makedirs(f'{model_type}/{cls_model_name}/data')\n",
    "\n",
    "    if not os.path.exists(f'{model_type}/{cls_model_name}/models'):\n",
    "        os.makedirs(f'{model_type}/{cls_model_name}/models')\n",
    "\n",
    "    if not os.path.exists(f'{ner_model_type}/{ner_model_name}/data'):\n",
    "        os.makedirs(f'{ner_model_type}/{ner_model_name}/data')\n",
    "\n",
    "    if not os.path.exists(f'{ner_model_type}/{ner_model_name}/models'):\n",
    "        os.makedirs(f'{ner_model_type}/{ner_model_name}/models')\n",
    "\n",
    "def query_data(db_type, sql_code):\n",
    "    # Get the input data we'll use in this analysis\n",
    "\n",
    "    query_file_name = db_type+'_query_results.csv'\n",
    "    if os.path.isfile(query_file_name):\n",
    "        df = pd.read_csv(query_file_name)\n",
    "    else:\n",
    "        query = sql_code\n",
    "        if db_type == 'redshift':\n",
    "            conn,cur = ut.redshift_conn()\n",
    "        elif db_type == 'snowflake':\n",
    "            conn,cur = ut.snowflake_conn()\n",
    "        df = ut.psql_table_query_df(cur,query,ucase_cols = False)\n",
    "        df.to_csv(query_file_name, index=False)\n",
    "        conn.close()\n",
    "    return df\n",
    "\n",
    "def generate_clust_input(ner_model_name, itemname_col, ner_clust_cols, df):\n",
    "    # Generate an input for our k-means clustering\n",
    "\n",
    "    # This code should be moved to nb2 or 3\n",
    "    # # If you ran notebooks 01 and 02, let's leverage the results from there. \n",
    "    # if os.path.isfile(f'named_entity_recognition/{ner_model_name}/data/{ner_model_name}_round1results.csv'):\n",
    "    #     ner_df = pd.read_csv(f'named_entity_recognition/{ner_model_name}/data/{ner_model_name}_round1results.csv')\n",
    "    #     ner_df = ner_df[[itemname_col] + ner_clust_cols].drop_duplicates()\n",
    "    #     df = pd.merge(df,ner_df,how = 'left')\n",
    "    #     df['clust_input'] = df[ner_clust_cols].fillna('').apply(lambda x: ' '.join(x), axis = 1).str.strip()\n",
    "        \n",
    "    # Let's use the full item name to cluster on\n",
    "    df['clust_input'] = df['item_for_selection']\n",
    "    return df\n",
    "\n",
    "create_folders_for_project(model_type, cls_model_name, ner_model_name)\n",
    "df = query_data(db_type, sql_code)\n",
    "df = df.loc[df['tier_2'].isin(['Breakfast', 'Entree'])] # This can be removed the next time the query is run\n",
    "df = clean_column_names(df)\n",
    "df = preprocess_item_name(df, itemname_col)\n",
    "df = generate_clust_input(ner_model_name, itemname_col, ner_clust_cols, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{model_type}/{cls_model_name}/data/{cls_model_name}_preprocessed.csv',index = False)\n",
    "df.to_csv(f'{ner_model_type}/{ner_model_name}/data/{ner_model_name}_preprocessed.csv',index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03bb224d",
   "metadata": {},
   "source": [
    "### Validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918d022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our output df has the right columns\n",
    "# If this fails, make sure that the columns tested for actually reside in your dataset\n",
    "\n",
    "cols = list(df)\n",
    "assert(all(item in cols for item in [itemname_col, spend_col, 'item_for_selection']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd04dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our output df has at least one row\n",
    "# If this fails, make sure your data wasn't accidentally dropped\n",
    "\n",
    "assert(df.shape[0]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debebbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared-sandbox",
   "language": "python",
   "name": "shared-sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "924cd68eab2f58ba959bdd6744e09bf0c48be24b286ae7351e60b0ee07a34664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
